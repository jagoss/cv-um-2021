{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Es7fW58WFzaQ"
   },
   "source": [
    "# Ejercicio 2: Face Detection\n",
    "\n",
    "Tal como se explica en la letra del obligatorio final en este ejercicio es necesario implementar desde cero una solucion para detección de caras. Se proveen datos de entrenamiento y es necesario implementar su propio algoritmo de sliding window para entrenar un clasificador. Todo el código necesario para comenzar a trabajar está provisto en este notebook.\n",
    "\n",
    "\n",
    "\\**En los ejercicios del trabajo final es posible utilizar funciones de librerias existentes o código sacado de internet. Siempre y cuando **no se usen para resolver explicitamente lo que pide el ejercicio** y al código sacado de interenet le agreguen el link en comentarios de donde fue sacado ese código.*\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iiKqFLGiG_GF"
   },
   "source": [
    "##### Imports necesarios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "executionInfo": {
     "elapsed": 4080,
     "status": "ok",
     "timestamp": 1624292707604,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "1R0bQDoWFlN8"
   },
   "outputs": [],
   "source": [
    "import cv2 \n",
    "import numpy as np\n",
    "from glob import glob\n",
    "from enum import Enum\n",
    "import os\n",
    "import sklearn \n",
    "import sklearn.neighbors\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from evaluation import *\n",
    "import sys\n",
    "#from google.colab.patches import cv2_imshow\n",
    "from image_utils import *\n",
    "#from imutils.object_detection import non_max_suppression\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage import feature #LBP\n",
    "from skimage.transform import resize #HoG\n",
    "from PIL import Image\n",
    "from skimage.feature import hog #HoG\n",
    "from skimage import data, exposure #HoG\n",
    "\n",
    "\n",
    "from sklearn.pipeline import make_pipeline #SVM\n",
    "from sklearn.preprocessing import StandardScaler #SVM\n",
    "from sklearn.svm import SVC #SVM\n",
    "from sklearn import svm ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ykAG8DVaHTBW"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dL0t1grWIgR_"
   },
   "source": [
    "#### Feature Extractors \n",
    "\n",
    "Para resolver el ejercicio van a tener que implementar las funciones `extract_hog_features` y `extract_lbp_features`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 271,
     "status": "ok",
     "timestamp": 1624292725821,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "JkDuSqQJIEUN"
   },
   "outputs": [],
   "source": [
    "class FeatureExtractors(Enum):\n",
    "    MiniImage = 1\n",
    "    HOG = 2\n",
    "    LBP = 3\n",
    "\n",
    "def extract_features(method, img):\n",
    "    '''Switch between Feature extraction Methods'''\n",
    "\n",
    "    image_representation = []\n",
    "\n",
    "    if method == FeatureExtractors.MiniImage:\n",
    "        image_representation = extract_mini_image_features(img)\n",
    "    elif method == FeatureExtractors.HOG:\n",
    "        image_representation = extract_hog_features(img)\n",
    "    elif method == FeatureExtractors.LBP:\n",
    "        image_representation = extract_lbp_features(img)\n",
    "\n",
    "    return image_representation\n",
    "\n",
    "def extract_mini_image_features(img,resize_size=(64,64)):\n",
    "    resized_image = cv2.resize(img,resize_size)\n",
    "    image_representation = resized_image.reshape(resize_size[0]*resize_size[1])\n",
    "    \n",
    "    return image_representation\n",
    "\n",
    "\n",
    "def extract_lbp_features(img):\n",
    "    radius = 3\n",
    "    n_points = 8 * radius\n",
    "    lbp = feature.local_binary_pattern(img, n_points,\n",
    "                                       radius, method=\"uniform\")\n",
    "    \n",
    "    return lbp.flatten()\n",
    "\n",
    "def extract_hog_features(img):\n",
    "    fd , hog_image = hog(img, orientations=16, pixels_per_cell=(8,8), cells_per_block=(8,8), visualize=True)\n",
    "    return fd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2548,
     "status": "ok",
     "timestamp": 1624292732162,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "CcLZST2DiUgK",
    "outputId": "fb2b976f-3fc9-47b9-f02b-fc3d84c8b38d"
   },
   "outputs": [],
   "source": [
    "img = cv2.imread('./data/face_detection/val_face_detection_images/unseen_Paris_Hilton_0001.jpg')\n",
    "img_gray =  cv2.cvtColor(img, cv2.COLOR_RGB2GRAY)\n",
    "img_hog = extract_features(FeatureExtractors.HOG, img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6lPvlmEnIs2n"
   },
   "source": [
    "#### Data loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 10,
     "status": "ok",
     "timestamp": 1624292733356,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "WX-Tpj9JI1qt"
   },
   "outputs": [],
   "source": [
    "def load_training_data(training_positive_dir,trainign_negative_dir, feature_extractor=FeatureExtractors.MiniImage):\n",
    "    ''' Function for loading loading training data from positive and negative examples\n",
    "    '''\n",
    "    positive_img_files = sorted(glob(training_positive_dir + '/*'))\n",
    "    negative_img_files = sorted(glob(trainign_negative_dir + '/*'))\n",
    "    #comment this line for loading all data\n",
    "    positive_img_files = positive_img_files[:100]\n",
    "    negative_img_files = negative_img_files[:200]\n",
    "\n",
    "    training_data = []\n",
    "    training_labels = []\n",
    "    \n",
    "    print('##Loading {} positive face images'.format(len(positive_img_files)))\n",
    "    for img in positive_img_files:\n",
    "        image = cv2.imread(img,cv2.IMREAD_GRAYSCALE)\n",
    "        image_representation = extract_features(feature_extractor,image)\n",
    "        training_data.append(image_representation)\n",
    "        training_labels.append(1)\n",
    "    \n",
    "    print('##Loading {} negative face images'.format(len(negative_img_files)))\n",
    "    for img in negative_img_files:\n",
    "        image = cv2.imread(img,cv2.IMREAD_GRAYSCALE)\n",
    "        image_representation = extract_features(feature_extractor,image)\n",
    "        training_data.append(image_representation)\n",
    "        training_labels.append(0)   \n",
    "    \n",
    "    training_data = np.asarray(training_data)\n",
    "    training_labels = np.asarray(training_labels)\n",
    "    return training_data, training_labels\n",
    "\n",
    "def load_validation_data(validation_data_dir):\n",
    "\n",
    "    validation_image_files = sorted(glob(validation_data_dir + '/*'))\n",
    "    val_images = []\n",
    "    for img_file in validation_image_files:\n",
    "        image = cv2.imread(img_file,cv2.IMREAD_COLOR)\n",
    "        val_images.append(image)\n",
    "\n",
    "    return val_images "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Rqb_HSDvJL9m"
   },
   "source": [
    "#### Sliding Window\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1624292736314,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "Jf1Ic0xYJghW"
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, window_size, scale, stride):\n",
    "    [image_rows, image_cols] = image.shape;\n",
    "    window_rows = window_size[0];\n",
    "    window_cols = window_size[1];\n",
    "\n",
    "    patches = np.zeros((window_rows, window_cols,5));\n",
    "    bbox_locations = np.zeros((5,4))\n",
    "    r = np.random.randint(0,image_rows-window_rows,5); # Sample top left position\n",
    "    c = np.random.randint(0,image_cols-window_cols,5);\n",
    "    for i in range(0,5):\n",
    "        patches[:,:,i] = image[r[i]:r[i]+window_rows, c[i]:c[i]+window_cols];\n",
    "        bbox_locations[i,:] = [r[i],c[i],window_rows,window_cols]; # top-left y,x, height, width\n",
    "\n",
    "    return patches, bbox_locations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AvgD3gp8LK2l"
   },
   "source": [
    "##### Metodos Auxiliares\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 287,
     "status": "ok",
     "timestamp": 1624292739188,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "A7dqYztRLcpg"
   },
   "outputs": [],
   "source": [
    "def show_image_with_bbox(image,bboxes,draw_GT=True):\n",
    "    GT = [82,91,166,175]\n",
    "    if draw_GT:\n",
    "        cv2.rectangle(image, (GT[0],GT[1]), (GT[2],GT[3]), (0, 0, 255), 2)\n",
    "\n",
    "    for bbox in bboxes:\n",
    "        if len(bbox) == 4:   \n",
    "            top_left = (int(bbox[0]),int(bbox[1]))\n",
    "            bottom_right = (int(bbox[0])+ int(bbox[2]),int(bbox[1])+int(bbox[3]))\n",
    "            cv2.rectangle(image, top_left, bottom_right, (255, 0, 0), 2)\n",
    "    plt.imshow(image[...,::-1])\n",
    "    plt.axis('off')\n",
    "    plt.show()\n",
    "    #cv2.imshow('image',image)\n",
    "    #cv2.waitKey(0) #wait for any key\n",
    "    #cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PZ3ZbWKKFlOD"
   },
   "source": [
    "### Ubicación de los datos ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 7,
     "status": "ok",
     "timestamp": 1624292744445,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "z-zVz-OIFlOF"
   },
   "outputs": [],
   "source": [
    "data_dir='./data'\n",
    "face_detection_dir = os.path.join(data_dir, 'face_detection')\n",
    "training_faces_dir = os.path.join(face_detection_dir,'cropped_faces')\n",
    "negative_examples_training_dir = os.path.join(face_detection_dir,'non_faces_images','neg_cropped_img')\n",
    "validation_faces_dir = os.path.join(face_detection_dir,'val_face_detection_images')\n",
    "validation_raw_faces_dir = os.path.join(face_detection_dir,'val_raw_images')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cU7x6hD5LUC7"
   },
   "source": [
    "## Entrenar Modelo y Face Detection\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fuo0icz8FlOG"
   },
   "source": [
    "### Cargar Datos de Entrenamiento ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 145698,
     "status": "ok",
     "timestamp": 1624292891392,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "DtE_MyFWFlOH",
    "outputId": "11683af4-f4d4-4569-9b85-19760b0a4afb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Loading 100 positive face images\n",
      "##Loading 200 negative face images\n"
     ]
    }
   ],
   "source": [
    "#Modify data_loader.py to load more training data\n",
    "training_data, trainig_labels = load_training_data(training_faces_dir,negative_examples_training_dir, FeatureExtractors.MiniImage)\n",
    "# You can save traninig_data and labels on nunmpy files to avoid processing data every time. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VE3nYFzmFlOI"
   },
   "source": [
    "### Load Validation Data ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 3705,
     "status": "ok",
     "timestamp": 1624293008545,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "4N00jneNFlOI"
   },
   "outputs": [],
   "source": [
    "validation_data = load_validation_data(validation_faces_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "l7poHeRhFlOI"
   },
   "source": [
    "### Entrenar un clasificador utilizando los datos de entrenamiento ## \n",
    "1. Una vez los datos de entrenamiento han sido cargados es necesario entrenar su propio clasificador \n",
    "2. Como solución inicial se utiliza un clasificador KNN pero para tener mejores resultados es posible entrenar un SVM\n",
    "3. Train your own classifier and save it on 'face_detector' file.Liblinear is suggested sample code for liblinear can be seen in './liblinear-2.30/python/README' file, good starting parameters for the classfier are '-s 2 -B 1'. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 269,
     "status": "ok",
     "timestamp": 1624293018431,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "oSpg1nVpFlOK"
   },
   "outputs": [],
   "source": [
    "knn_classifier = sklearn.neighbors.KNeighborsClassifier(n_neighbors=3) #CAMBIAR POR UN SVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 524,
     "status": "ok",
     "timestamp": 1624293020202,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "pIQwPwWQFlOM",
    "outputId": "2737cd8b-f46b-497a-a1ce-6ddb99a6a5be"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(n_neighbors=3)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_classifier.fit(training_data,trainig_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1355,
     "status": "ok",
     "timestamp": 1624293024282,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "IvPcnmmNRJM6",
    "outputId": "7b740413-1db3-473c-a6d4-859265b764ae"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SVC(probability=True)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf = svm.SVC(C=1.0,probability=True)\n",
    "clf.fit(training_data,trainig_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1s2k3ufmFlOQ"
   },
   "source": [
    "#### Guardar el modelo entrenado ##"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 1112,
     "status": "ok",
     "timestamp": 1624293028792,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "2fRvvaBOFlOR"
   },
   "outputs": [],
   "source": [
    "pickle.dump(knn_classifier,open('./face_detector', 'wb'))\n",
    "pickle.dump(clf,open('./face_detector', 'wb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xWUPszrDFlOS"
   },
   "source": [
    "#### Cargar el Modelo entrenado "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 275,
     "status": "ok",
     "timestamp": 1624293032572,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "KVizcW8qFlOS"
   },
   "outputs": [],
   "source": [
    "classifier = pickle.load(open('./face_detector','rb'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 2258,
     "status": "ok",
     "timestamp": 1624293036305,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "gpOO75voAh2r",
    "outputId": "c45d8628-cc51-4794-8852-f0bcdf0d01a4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "##Loading 100 positive face images\n",
      "##Loading 200 negative face images\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(steps=[('standardscaler', StandardScaler()),\n",
       "                ('svc', SVC(gamma='auto'))])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "training_data, trainig_labels = load_training_data(training_faces_dir,negative_examples_training_dir, FeatureExtractors.LBP)\n",
    "clf = make_pipeline(StandardScaler(), SVC(gamma='auto'))\n",
    "clf.fit(training_data, trainig_labels)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4JLCn_hOFlOS"
   },
   "source": [
    "### Do the sliding window and Visualize Results ###\n",
    "1. In this part you need to perform the sliding window and score the probabilty of each patch of being a face and select the highest probability patches. \n",
    "    1a. Program you own sliding window in 'img_utils' file.\n",
    "    1b. Extract Features and classify each patch with your own classifier. \n",
    "2. Do non-max suppression to select the target face patch with best probability (This is provided in 'img_utils')\n",
    "3. Visualize Detection with Ground Throuth Bounding Box ('Code for Visualization is Provided')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "executionInfo": {
     "elapsed": 351,
     "status": "ok",
     "timestamp": 1624293041113,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "dCl16FR6Haxf"
   },
   "outputs": [],
   "source": [
    "def sliding_window(image, stepSize, windowSize):\n",
    "    [image_rows, image_cols] = image.shape\n",
    "    window_rows = windowSize[0];\n",
    "    window_cols = windowSize[1];\n",
    "    patches = np.zeros((windowSize[0], windowSize[1],(image_cols-window_cols)*(image_rows-window_rows)))                                              \n",
    "    bbox_locations = np.zeros(((image_cols-window_cols)*(image_rows-window_rows),4))                                                                  \n",
    "    i = 0\n",
    "    # slide a window across the image\n",
    "    range_y = int((image_cols-window_cols)/stepSize)\n",
    "    range_x = int((image_rows-window_rows)/stepSize)\n",
    "    \n",
    "    for y in range(0, range_y, stepSize):\n",
    "      for x in range(0, range_x, stepSize):\n",
    "        patches[:,:,i] = image[x:x+window_rows, y:y+window_cols]\n",
    "        bbox_locations[i,:] = [x, y, window_rows, window_cols]\n",
    "        i+=1\n",
    "    return patches, bbox_locations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 1516311,
     "status": "ok",
     "timestamp": 1624294560903,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "8ceFJnT3FlOT",
    "outputId": "99087c1b-4914-48d0-fffc-7ca8be2fd916"
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "X.shape[1] = 1024 should be equal to 4096, the number of features at training time",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-c4bd5f33a1f7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0mpatches_feature_representation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches_feature_representation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0;31m## Get prediction label for each sliding window patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 18\u001b[0;31m     \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches_feature_representation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     19\u001b[0m     \u001b[0;31m## Get score for each sliding window patch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpatches_feature_representation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    622\u001b[0m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdecision_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    623\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 624\u001b[0;31m             \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    625\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclasses_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mintp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    626\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    340\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0mndarray\u001b[0m \u001b[0mof\u001b[0m \u001b[0mshape\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mn_samples\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    341\u001b[0m         \"\"\"\n\u001b[0;32m--> 342\u001b[0;31m         \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_for_predict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    343\u001b[0m         \u001b[0mpredict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse_predict\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sparse\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dense_predict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.8/site-packages/sklearn/svm/_base.py\u001b[0m in \u001b[0;36m_validate_for_predict\u001b[0;34m(self, X)\u001b[0m\n\u001b[1;32m    491\u001b[0m                                  (X.shape[1], self.shape_fit_[0]))\n\u001b[1;32m    492\u001b[0m         \u001b[0;32melif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mcallable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkernel\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape_fit_\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 493\u001b[0;31m             raise ValueError(\"X.shape[1] = %d should be equal to %d, \"\n\u001b[0m\u001b[1;32m    494\u001b[0m                              \u001b[0;34m\"the number of features at training time\"\u001b[0m \u001b[0;34m%\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    495\u001b[0m                              (X.shape[1], self.shape_fit_[1]))\n",
      "\u001b[0;31mValueError\u001b[0m: X.shape[1] = 1024 should be equal to 4096, the number of features at training time"
     ]
    }
   ],
   "source": [
    "window_size = [64, 64]\n",
    "predictions = []\n",
    "threshold_p = 0.8    #\n",
    "overlap_threshold = 0.5     #\n",
    "validation_data = load_validation_data(validation_faces_dir)\n",
    "for img in validation_data:\n",
    "    gray_image = cv2.cvtColor(img,cv2.COLOR_RGB2GRAY)\n",
    "    #patches, bbox_locations = sliding_window(gray_image,window_size,1,32)     #\n",
    "    patches, bbox_locations = sliding_window(gray_image,stepSize=32, windowSize=(64, 64))     #Nuestra\n",
    "    \n",
    "    ## You need to extract features for every patch (same features you used for training the classifier)\n",
    "    patches_feature_representation = []\n",
    "    for i in range(patches.shape[2]):\n",
    "        patch_representation = extract_features(FeatureExtractors.HOG, patches[:,:,i])\n",
    "        patches_feature_representation.append(patch_representation)\n",
    "    patches_feature_representation = np.asarray(patches_feature_representation)\n",
    "    ## Get prediction label for each sliding window patch\n",
    "    labels = classifier.predict(patches_feature_representation)\n",
    "    ## Get score for each sliding window patch\n",
    "    scores = classifier.predict_proba(patches_feature_representation)\n",
    "    ## Positive Face Probabilities\n",
    "    face_probabilities = scores[:,1]\n",
    "    face_bboxes = bbox_locations[face_probabilities>threshold_p]\n",
    "    face_bboxes_probabilites = face_probabilities[face_probabilities>threshold_p]\n",
    "    # Do non max suppression and select strongest probability box\n",
    "    [selected_bbox, selected_score] = non_max_suppression(face_bboxes,face_bboxes_probabilites)\n",
    "    show_image_with_bbox(img, selected_bbox)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aTKhHhrcFlOV"
   },
   "source": [
    "### Evaluate Detector ###"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 309,
     "status": "ok",
     "timestamp": 1624284419675,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "tfoUPIDKpa1c"
   },
   "outputs": [],
   "source": [
    "def evaluate_detector(bboxes, positive_probabilites):\n",
    "    \n",
    "    true_positives_number = np.zeros((100))\n",
    "    actual_positives = np.zeros((100))\n",
    "    predicted_positives = np.zeros((100))\n",
    "    overlap_threshold = 0.5\n",
    "\n",
    "    for i in np.arange(0,1,0.01):\n",
    "        probability_threshold = i\n",
    "        positive_bboxes = bboxes[positive_probabilites>=probability_threshold]\n",
    "        positive_bboxes_prob = positive_probabilites[positive_probabilites>=probability_threshold]\n",
    "        idx = int(np.round(i*100))\n",
    "        if len(positive_bboxes) > 0:\n",
    "            [selected_bboxes, selected_scores] = non_max_suppression(positive_bboxes, positive_bboxes_prob,0.3)\n",
    "            ratio = []\n",
    "            for selected_bbox in selected_bboxes:\n",
    "                ratio.append(bb_intersection_over_union(selected_bbox, [82,91,84,84]));                                   \n",
    "            \n",
    "            ratio = np.asarray(ratio)\n",
    "            positive_number = sum(ratio>=0.5); \n",
    "            \n",
    "            true_positives_number[idx] = positive_number>=1;\n",
    "            actual_positives[idx] = 1\n",
    "            predicted_positives[idx] = len(ratio)\n",
    "\n",
    "        else:\n",
    "            true_positives_number[idx] = 0;\n",
    "            actual_positives[idx] = 1\n",
    "            predicted_positives[idx] = 0\n",
    "\n",
    "    return [true_positives_number, actual_positives, predicted_positives]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N_uqzQvfFlOW"
   },
   "outputs": [],
   "source": [
    "total_true_positives = []\n",
    "total_real_positives = []\n",
    "total_true_prediction = []\n",
    "window_size = [64, 64]\n",
    "score_threshold = 0.5\n",
    "sum_boxes=0\n",
    "idx=0\n",
    "for subject_folder in sorted(glob(validation_raw_faces_dir + '/*')):\n",
    "    for img in sorted(glob(subject_folder + '/*.jpg')):\n",
    "        idx+=1\n",
    "        gray_image = cv2.imread(img,cv2.IMREAD_GRAYSCALE)\n",
    "        #patches, bbox_locations = sliding_window(gray_image,window_size,1,32)\n",
    "        \n",
    "        patches, bbox_locations = sliding_window(gray_image,stepSize=32, windowSize=(64, 64))     #Nuestra\n",
    "    \n",
    "        \n",
    "        ## You need to extract features for every patch (same features you used for training the classifier)\n",
    "        patches_feature_representation = []\n",
    "        for i in range(patches.shape[2]):\n",
    "            patch_representation = extract_features(FeatureExtractors.HOG, patches[:,:,i])\n",
    "            patches_feature_representation.append(patch_representation)\n",
    "        patches_feature_representation = np.asarray(patches_feature_representation)\n",
    "        ## Get score for each sliding window patch\n",
    "        scores = classifier.predict_proba(patches_feature_representation)\n",
    "        ## Positive Face Probabilities\n",
    "        face_probabilities = scores[:,1]\n",
    "        ## Filter boxes by probability or score KNN probability > 0.5 or SVM score > 0\n",
    "        [ detected_true_positives, image_real_positives, detected_faces ] = evaluate_detector( bbox_locations, face_probabilities );\n",
    "        \n",
    "        total_true_positives.append(detected_true_positives)\n",
    "        total_real_positives.append(image_real_positives)\n",
    "        total_true_prediction.append(detected_faces)\n",
    "\n",
    "total_true_positives = np.asarray(total_true_positives)\n",
    "total_real_positives = np.asarray(total_real_positives)\n",
    "total_true_prediction = np.asarray(total_true_prediction)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 264,
     "status": "ok",
     "timestamp": 1624137572069,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "HJMARGMBpa1e",
    "outputId": "9ae85868-57b7-423a-be58-cd469f51c613"
   },
   "outputs": [],
   "source": [
    "precision, recall = precision_and_recall(total_true_positives, total_real_positives,total_true_prediction)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 297
    },
    "executionInfo": {
     "elapsed": 816,
     "status": "ok",
     "timestamp": 1624137574398,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "wQeR6tmapa1e",
    "outputId": "aa1831ef-e951-4402-970c-f04fdfefa9db"
   },
   "outputs": [],
   "source": [
    "plt.plot(recall, precision)\n",
    "plt.xlabel('Recall')\n",
    "plt.ylabel('Precision')\n",
    "plt.xlim(0,1.1)\n",
    "plt.ylim(0,1.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Yw74Qp4VFlOc"
   },
   "outputs": [],
   "source": [
    "ap = interpolated_average_precision(recall,precision)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 258,
     "status": "ok",
     "timestamp": 1624137577837,
     "user": {
      "displayName": "Sofi Iriarte",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GioMxMvDPunfqWR0E_gPE-Vp2pUusk1wVnyJ4Qn=s64",
      "userId": "15725192827013080256"
     },
     "user_tz": 180
    },
    "id": "dBVw-MBqFlOc",
    "outputId": "7e8c4642-e401-42bd-b8a5-eb2afa4fc5f6"
   },
   "outputs": [],
   "source": [
    "print('Detection Average Precision is {}'.format(ap))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "7vbeMuOXHaZE"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BpBphdvSHxY2"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AydAXc_uICzG"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "523YswDwJEQe"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "L0H9CS0OJHf-"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLrybjkfLPJb"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "Face_Detection.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
